{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FallDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JI1ExEjqkAJp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonercool/deep-learning-exam-project/blob/master/FallDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MutW9icTjgAZ"
      },
      "source": [
        "# Deep Learning course project - Fall Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI1ExEjqkAJp"
      },
      "source": [
        "###Group members\n",
        "\n",
        "\n",
        "*   Anton Sihm, studentId.: 201504954\n",
        "*   Peter Marcus Hoveling, studentId.: 201508876\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK6SxF2bk1mh"
      },
      "source": [
        "\n",
        "###Purpose\n",
        "\n",
        "This project aims to detect human actions from live video feed, in order to achieve safety, such as\n",
        "alarms when elderly people fall in their apartments.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVy_WAXzlHvr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5u6jEplkP8u"
      },
      "source": [
        "### Data sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mxgGvdFkd02"
      },
      "source": [
        "To evaluate the fall detection, dataset from realistic video surveillance settings will be used. Framerate\n",
        "and resolution may vary as finding the same formatted video data is challenging\n",
        "\n",
        "\n",
        "*   mViA - Fall detection dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbMKOj_WtAAT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfgWtudgrvZH"
      },
      "source": [
        "## Task 0: Mounting Google Drive data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNqOf05KhNV-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9095795b-423f-4452-b38f-3ee2279dc49c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZUArads4-R"
      },
      "source": [
        "Creating a variable to the specific files mouted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88QdPuwfsoZF"
      },
      "source": [
        "from pathlib import Path\n",
        "root = '/content/gdrive/My Drive/' # Don't change this\n",
        "data_dirname = 'Data_complete' # Change as you like\n",
        "p = Path(root + data_dirname)\n",
        "p.mkdir(exist_ok=True) # should \"/content/gdrive/My Drive/Data\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t--_-SBqtFTZ"
      },
      "source": [
        "Validating the data directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiApPSd8tDwI",
        "outputId": "7ce626f9-3e85-47ed-aa4c-cdf21bf07fbf"
      },
      "source": [
        "print(p)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Data_complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-MykY4QsBe0"
      },
      "source": [
        "## Task 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s2kqYj19bi"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n6NzYQm2EwD"
      },
      "source": [
        "# Importing all necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy \n",
        "import random\n",
        "\n",
        "class VideoUtils:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def convert_single_video_to_images(self, video_path, annotation_path):\n",
        "    # Read the video from specified path\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    meta_data = open(annotation_path)\n",
        "\n",
        "    start_fall_frame = meta_data.readline() \n",
        "    end_fall_frame = meta_data.readline() \n",
        "\n",
        "    not_falling = False\n",
        "    # check if has labels\n",
        "    if \",\" in start_fall_frame:\n",
        "      print(f\"does not contain : {annotation_path}\")\n",
        "      ## skip current\n",
        "      not_falling = True\n",
        "      start_fall_frame = 0\n",
        "      end_fall_frame = 0\n",
        "    else:\n",
        "      start_fall_frame = int(start_fall_frame)\n",
        "      end_fall_frame = int(end_fall_frame)  \n",
        "\n",
        "    traning_samples = []\n",
        "    training_labels = []\n",
        "\n",
        "    currentframe = 0\n",
        "\n",
        "    while(True):\n",
        "\n",
        "      # reading from frame\n",
        "      ret,frame = video.read()\n",
        "      if ret:\n",
        "        traning_samples.append(frame)\n",
        "\n",
        "        if not_falling == False:\n",
        "          # if falling - label 1\n",
        "          if currentframe >= start_fall_frame and currentframe <= end_fall_frame:\n",
        "            training_labels.append(1)\n",
        "          # not falling - label 0\n",
        "          else:\n",
        "            training_labels.append(0)\n",
        "        else:\n",
        "          training_labels.append(0)\n",
        "\n",
        "        # increasing counter so that it will\n",
        "        # show how many frames are created\n",
        "        currentframe += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    # Release all space and windows once done\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    traning_samples = numpy.array(traning_samples)\n",
        "    training_labels =  numpy.array(training_labels)\n",
        "\n",
        "    return (traning_samples, training_labels, True)\n",
        "\n",
        "\n",
        "  def _get_movie_IDs(self, root_path):\n",
        "    list_video_paths = []\n",
        "    annotation_path = r\"/Annotation_files\"\n",
        "    video_path = r\"/Videos\"\n",
        "    \n",
        "    data_sub_dir = os.listdir(root_path)\n",
        "\n",
        "    # interate each sub dir\n",
        "    for dir in data_sub_dir:\n",
        "      if \".ipynb\" in dir:\n",
        "        continue\n",
        "      # for each file in sub dir\n",
        "      for file in os.listdir(str(root_path) + \"/\" + dir + video_path):\n",
        "        absolute_video_file_path = str(str(root_path) + \"/\" + dir + video_path + \"/\"+ file)\n",
        "        list_video_paths.append(absolute_video_file_path)\n",
        "\n",
        "    return list_video_paths\n",
        "\n",
        "\n",
        "  # create_partition_list(p)\n",
        "  def create_partition_list(self, root_path, traning_percentage=80, validation_percentage=20):\n",
        "    partition = {}\n",
        "\n",
        "    absolute_video_path_list = video_utils._get_movie_IDs(root_path)\n",
        "    random.shuffle(absolute_video_path_list)\n",
        "\n",
        "    to_index = int((len(absolute_video_path_list)+1)*(traning_percentage/100))\n",
        "    from_index = int(len(absolute_video_path_list)*(traning_percentage/100)+1)\n",
        "\n",
        "    print(f\"traning {0}:{to_index}\")\n",
        "    print(f\"validation {from_index}:{len(absolute_video_path_list)}\")\n",
        "\n",
        "    training = absolute_video_path_list[0:to_index]\n",
        "    validation = absolute_video_path_list[from_index:]\n",
        "\n",
        "    total_frames_training = 0\n",
        "    total_frames_validation = 0\n",
        "    \n",
        "    for image_path in training:\n",
        "      cap= cv2.VideoCapture(image_path)\n",
        "      total_frames_training += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    for image_path in validation:\n",
        "      cap= cv2.VideoCapture(image_path)\n",
        "      total_frames_validation += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    \n",
        "    partition['Training'] = (training, total_frames_training)\n",
        "    partition['Validation'] = (validation, total_frames_validation)\n",
        "\n",
        "    return partition\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPyh4Iw-1Ued"
      },
      "source": [
        "### Testing Video ultils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxj08t9B1a3P",
        "outputId": "4b73f02b-8fb1-4656-f89d-c591794c9260"
      },
      "source": [
        "video_utils = VideoUtils()\n",
        "traning_samples, training_labels, is_ok = video_utils.convert_single_video_to_images(str(p) + \"/Office/Videos/video (6).avi\", str(p) + \"/Office/Annotation_files/video (6).txt\")\n",
        "print(traning_samples.shape)\n",
        "\n",
        "absolute_video_path_list = video_utils._get_movie_IDs(p)\n",
        "\n",
        "partition = video_utils.create_partition_list(p)\n",
        "\n",
        "print(partition['Training'][1])\n",
        "print(partition['Validation'][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(415, 240, 320, 3)\n",
            "traning 0:152\n",
            "validation 153:190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb8tV0PSkg1x"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLVPgdLgkodl"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "class FallingDataGenerator(Sequence):\n",
        "\n",
        "  def __init__(self, movie_path_list, total_movie_frames, batch_size=32, dim=(240, 320), n_channels=3, n_classes=2, shuffle=True): \n",
        "      'Initialization'\n",
        "      self.dim = dim\n",
        "      self.batch_size = batch_size\n",
        "      self.n_channels = n_channels\n",
        "      self.n_classes = n_classes\n",
        "      self.shuffle = shuffle\n",
        "      self.movie_path_list = movie_path_list\n",
        "      self.total_movie_frames = total_movie_frames\n",
        "      self.current_video_data = 0# ((415, 240, 320, 3)(415)) # 0-32, 32-64 ...... 400-415    \n",
        "      self.current_video_frame = 0\n",
        "\n",
        "      # init by shuffle\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  # Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike\n",
        "  ## TODO shuffle path list\n",
        "  def on_epoch_end(self):\n",
        "    'Updates indexes after each epoch'\n",
        "    self.indexes = np.arange(len(self.movie_path_list))\n",
        "    if self.shuffle == True:\n",
        "        np.random.shuffle(self.indexes)\n",
        "\n",
        "  def get_indexes(self):\n",
        "    return self.indexes\n",
        "\n",
        "  def __data_generation(self, list_IDs_temp):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    # Initialization\n",
        "    X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "    y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "    ## //TODO Can use multiple cores here\n",
        "    # Generate data\n",
        "    #for i, ID in enumerate(list_IDs_temp):\n",
        "    #    # Store sample\n",
        "    #    X[i,] = np.load('data/' + ID + '.npy')\n",
        "#\n",
        "    #    # Store class\n",
        "    #    y[i] = self.labels[ID]\n",
        "\n",
        "    return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of batches per epoch'\n",
        "    ## Total number of frames in all videos / batchsize\n",
        "    return int(np.floor(len(self.total_movie_frames) / self.batch_size))\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generate one batch of data'\n",
        "    # Generate indexes of the batch\n",
        "    indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "    # Find list of IDs\n",
        "    #list_IDs_temp = [self.movie_path_list[k] for k in indexes]\n",
        "\n",
        "    # Generate data\n",
        "    X, y = self.__data_generation(movie_path_list)\n",
        "\n",
        "    return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5By3K9GZA9zF"
      },
      "source": [
        "### Testing data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRllNJscA_wr"
      },
      "source": [
        "# Parameters\n",
        "params = {'batch_size': 32,\n",
        "          'dim': (240, 320),\n",
        "          'n_channels': 3,\n",
        "          'n_classes': 2, \n",
        "          'shuffle': True}\n",
        "\n",
        "#utils_video\n",
        "video_utils = VideoUtils()\n",
        "\n",
        "# Datasets\n",
        "partition = video_utils.create_partition_list(p)\n",
        "\n",
        "\n",
        "# Generators\n",
        "training_generator = FallingDataGenerator(partition['Training'][0], partition['Training'][1], **params)\n",
        "validation_generator = FallingDataGenerator(partition['Validation'][0], partition['Validation'][1], **params)\n",
        "\n",
        "#print(len(training_generator.get_indexes()))\n",
        "#training_generator.on_epoch_end()\n",
        "\n",
        "#print(training_generator.__len__())\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}