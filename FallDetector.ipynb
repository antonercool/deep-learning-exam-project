{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FallDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JI1ExEjqkAJp"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonercool/deep-learning-exam-project/blob/master/FallDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuijswKcNnPY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MutW9icTjgAZ"
      },
      "source": [
        "# Deep Learning course project - Fall Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI1ExEjqkAJp"
      },
      "source": [
        "###Group members\n",
        "\n",
        "\n",
        "*   Anton Sihm, studentId.: 201504954\n",
        "*   Peter Marcus Hoveling, studentId.: 201508876\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK6SxF2bk1mh"
      },
      "source": [
        "\n",
        "###Purpose\n",
        "\n",
        "This project aims to detect human actions from live video feed, in order to achieve safety, such as\n",
        "alarms when elderly people fall in their apartments.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVy_WAXzlHvr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5u6jEplkP8u"
      },
      "source": [
        "### Data sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mxgGvdFkd02"
      },
      "source": [
        "To evaluate the fall detection, dataset from realistic video surveillance settings will be used. Framerate\n",
        "and resolution may vary as finding the same formatted video data is challenging\n",
        "\n",
        "\n",
        "*   mViA - Fall detection dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbMKOj_WtAAT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfgWtudgrvZH"
      },
      "source": [
        "## Task 0: Mounting Google Drive data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNqOf05KhNV-",
        "outputId": "e59a6681-a1cb-415e-aa6e-93ac49b0368a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZUArads4-R"
      },
      "source": [
        "Creating a variable to the specific files mouted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88QdPuwfsoZF"
      },
      "source": [
        "from pathlib import Path\n",
        "root = '/content/gdrive/My Drive/' # Don't change this\n",
        "data_dirname = 'Data_complete' # Change as you like\n",
        "p = Path(root + data_dirname)\n",
        "p.mkdir(exist_ok=True) # should \"/content/gdrive/My Drive/Data\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t--_-SBqtFTZ"
      },
      "source": [
        "Validating the data directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiApPSd8tDwI",
        "outputId": "2498b7f1-d053-427a-b038-906f817c8c21"
      },
      "source": [
        "print(p)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Data_complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-MykY4QsBe0"
      },
      "source": [
        "## Task 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s2kqYj19bi"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n6NzYQm2EwD"
      },
      "source": [
        "# Importing all necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy \n",
        "import random\n",
        "\n",
        "class VideoUtils:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def convert_single_video_to_images(self, video_path, annotation_path, dim):\n",
        "    # Read the video from specified path\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    meta_data = open(annotation_path)\n",
        "\n",
        "    start_fall_frame = meta_data.readline() \n",
        "    end_fall_frame = meta_data.readline() \n",
        "\n",
        "    not_falling = False\n",
        "    # check if has labels\n",
        "    if \",\" in start_fall_frame:\n",
        "      #print(f\"does not contain : {annotation_path}\")\n",
        "      ## skip current\n",
        "      not_falling = True\n",
        "      start_fall_frame = 0\n",
        "      end_fall_frame = 0\n",
        "    else:\n",
        "      start_fall_frame = int(start_fall_frame)\n",
        "      end_fall_frame = int(end_fall_frame)  \n",
        "\n",
        "    traning_samples = []\n",
        "    training_labels = []\n",
        "\n",
        "    currentframe = 0\n",
        "\n",
        "    while(True):\n",
        "\n",
        "      # reading from frame\n",
        "      ret,frame = video.read()\n",
        "      if ret:\n",
        "        if(frame.shape[0] != dim[0] or frame.shape[1] != dim[1]):\n",
        "          try:\n",
        "            frame = self.center_crop(frame, dim)\n",
        "          except e:\n",
        "            print(f\"failed to center from image from vid : {video_path}, error {e}\")\n",
        "\n",
        "        traning_samples.append(frame)\n",
        "\n",
        "        if not_falling == False:\n",
        "          # if falling - label 1\n",
        "          if currentframe >= start_fall_frame and currentframe <= end_fall_frame:\n",
        "            training_labels.append(1)\n",
        "          # not falling - label 0\n",
        "          else:\n",
        "            training_labels.append(0)\n",
        "        else:\n",
        "          training_labels.append(0)\n",
        "\n",
        "        # increasing counter so that it will\n",
        "        # show how many frames are created\n",
        "        currentframe += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    # Release all space and windows once done\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    traning_samples = numpy.array(traning_samples)\n",
        "    training_labels =  numpy.array(training_labels)\n",
        "\n",
        "    return (traning_samples, training_labels, True)\n",
        "\n",
        "  # Function from: https://medium.com/curious-manava/center-crop-and-scaling-in-opencv-using-python-279c1bb77c74\n",
        "  def center_crop(self, img, dim):\n",
        "\t  \"\"\"Returns center cropped image\n",
        "\t  Args:\n",
        "\t  img: image to be center cropped\n",
        "\t  dim: dimensions (width, height) to be cropped\n",
        "\t  \"\"\"\n",
        "\t  width, height = img.shape[1], img.shape[0]\n",
        "\n",
        "\t  # process crop width and height for max available dimension\n",
        "\t  crop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n",
        "\t  crop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0] \n",
        "\t  mid_x, mid_y = int(width/2), int(height/2)\n",
        "\t  cw2, ch2 = int(crop_width/2), int(crop_height/2) \n",
        "\t  crop_img = img[mid_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n",
        "\t  return crop_img\n",
        "\n",
        "\n",
        "  def _get_movie_IDs(self, root_path):\n",
        "    list_video_paths = []\n",
        "    annotation_path = r\"/Annotation_files\"\n",
        "    video_path = r\"/Videos\"\n",
        "    \n",
        "    data_sub_dir = os.listdir(root_path)\n",
        "\n",
        "    # interate each sub dir\n",
        "    for dir in data_sub_dir:\n",
        "      if \".ipynb\" in dir:\n",
        "        continue\n",
        "      # for each file in sub dir\n",
        "      for file in os.listdir(str(root_path) + \"/\" + dir + video_path):\n",
        "        absolute_video_file_path = str(str(root_path) + \"/\" + dir + video_path + \"/\"+ file)\n",
        "        list_video_paths.append(absolute_video_file_path)\n",
        "\n",
        "    return list_video_paths\n",
        "\n",
        "\n",
        "  # create_partition_list(p)\n",
        "  def create_partition_list(self, root_path, traning_percentage=80, validation_percentage=20):\n",
        "    partition = {}\n",
        "\n",
        "    absolute_video_path_list = video_utils._get_movie_IDs(root_path)\n",
        "    random.shuffle(absolute_video_path_list)\n",
        "\n",
        "    to_index = int((len(absolute_video_path_list)+1)*(traning_percentage/100))\n",
        "    from_index = int(len(absolute_video_path_list)*(traning_percentage/100)+1)\n",
        "\n",
        "    print(f\"traning {0}:{to_index}\")\n",
        "    print(f\"validation {from_index}:{len(absolute_video_path_list)}\")\n",
        "\n",
        "    training = absolute_video_path_list[0:to_index]\n",
        "    validation = absolute_video_path_list[from_index:]\n",
        "\n",
        "    total_frames_training = 0\n",
        "    total_frames_validation = 0\n",
        "    \n",
        "    for image_path in training:\n",
        "      cap= cv2.VideoCapture(image_path)\n",
        "      total_frames_training += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    for image_path in validation:\n",
        "      cap= cv2.VideoCapture(image_path)\n",
        "      total_frames_validation += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    \n",
        "    partition['Training'] = (training, total_frames_training)\n",
        "    partition['Validation'] = (validation, total_frames_validation)\n",
        "\n",
        "    return partition"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPyh4Iw-1Ued"
      },
      "source": [
        "### Testing Video ultils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "yxj08t9B1a3P",
        "outputId": "79080baf-cd20-42b8-a92f-e4ccdb7bf3b1"
      },
      "source": [
        "video_utils = VideoUtils()\n",
        "traning_samples, training_labels, is_ok = video_utils.convert_single_video_to_images(str(p) + \"/Office/Videos/video (6).avi\", str(p) + \"/Office/Annotation_files/video (6).txt\")\n",
        "print(traning_samples.shape)\n",
        "\n",
        "absolute_video_path_list = video_utils._get_movie_IDs(p)\n",
        "\n",
        "partition = video_utils.create_partition_list(p)\n",
        "\n",
        "print(partition['Training'][1])\n",
        "print(partition['Validation'][1])\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f1fa6a9008ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideo_utils\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoUtils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraning_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_ok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_single_video_to_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Office/Videos/video (6).avi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/Office/Annotation_files/video (6).txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraning_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mabsolute_video_path_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_movie_IDs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: convert_single_video_to_images() missing 1 required positional argument: 'dim'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb8tV0PSkg1x"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLVPgdLgkodl"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from keras import utils as np_utils\n",
        "import random\n",
        "\n",
        "class FallingDataGenerator(Sequence):\n",
        "\n",
        "  def __init__(self, movie_path_list, total_movie_frames, batch_size=32, dim=(240, 320), n_channels=3, n_classes=2, shuffle=True): \n",
        "      'Initialization'\n",
        "      self.dim = dim\n",
        "      self.batch_size = batch_size\n",
        "      self.n_channels = n_channels\n",
        "      self.n_classes = n_classes\n",
        "      self.shuffle = shuffle\n",
        "      self.movie_path_list = movie_path_list\n",
        "      self.total_movie_frames = total_movie_frames\n",
        "      self.current_video_data = 0# ((415, 240, 320, 3)(415)) # 0-32, 32-64 ...... 400-415    \n",
        "      self.current_video_frame = 0\n",
        "      self.current_movie_index = 0\n",
        "      self.video_utils = VideoUtils()\n",
        "      self.current_batch = 0\n",
        "\n",
        "      # init by shuffle\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  # Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike\n",
        "  ## TODO shuffle path list\n",
        "  def on_epoch_end(self):\n",
        "    'Updates indexes after each epoch'\n",
        "    print(\"DEBUG - on epoch end\")\n",
        "    random.shuffle(self.movie_path_list)\n",
        "    self.current_video_data = 0\n",
        "    self.current_video_frame = 0\n",
        "    self.current_movie_index = 0\n",
        "\n",
        "\n",
        "  def get_indexes(self):\n",
        "    return self.indexes\n",
        "\n",
        "  def __data_generation(self):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    # Initialization\n",
        "    X = np.zeros((self.batch_size, *self.dim, self.n_channels)) # (32, H, W, 3)\n",
        "    Y = np.zeros((self.batch_size), dtype=int)\n",
        "\n",
        "    ## Init - first time load\n",
        "    if(self.current_video_data == 0 and self.current_video_frame == 0 and self.current_movie_index == 0):\n",
        "      #####print(\"DEBUG - First time load\")\n",
        "      video_path = self.movie_path_list[self.current_movie_index]\n",
        "      annotation_path = video_path.replace(\"Videos\", \"Annotation_files\")\n",
        "      annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "      self.current_video_data = self.video_utils.convert_single_video_to_images(video_path, annotation_path, self.dim)\n",
        "    \n",
        "    total_number_of_current_images = len(self.current_video_data[0])\n",
        "    #####print(f\"DEBUG - total_number_of_current_images : {total_number_of_current_images}\")\n",
        "\n",
        "    # Check if dont have enough frames\n",
        "    if(self.current_video_frame + self.batch_size + 1 > total_number_of_current_images):\n",
        "      #####print(f\"DEBUG - not have enough frames{self.current_video_frame + self.batch_size -1}>{total_number_of_current_images} - taking from current and current + 1\")\n",
        "      # How many frames did we overextend with\n",
        "      residual_value_from_current_video = (self.current_video_frame + self.batch_size)%total_number_of_current_images\n",
        "      # How many frames are we missing to complete current video\n",
        "      frames_missing = self.batch_size - residual_value_from_current_video\n",
        "\n",
        "      # loop for the remaining needed frames\n",
        "      for i in range(frames_missing):\n",
        "        #####print(f\"DEBUG - taking frame_index: {self.current_video_frame}\")\n",
        "        X[i,] = self.current_video_data[0][self.current_video_frame]\n",
        "        Y[i,] = self.current_video_data[1][self.current_video_frame]\n",
        "        self.current_video_frame += 1\n",
        "    \n",
        "      # Continue to next video\n",
        "      self.current_movie_index += 1\n",
        "      self.current_video_frame = 0\n",
        "      self.current_video_data = 0\n",
        "\n",
        "      # If we have more movies\n",
        "      # 0, 1, 2     -- 2<=2\n",
        "      if(self.current_movie_index <= (len(self.movie_path_list) - 1)):\n",
        "        #####print(\"DEBUG - taking new vid\")\n",
        "        video_path = self.movie_path_list[self.current_movie_index]\n",
        "        annotation_path = video_path.replace(\"Videos\", \"Annotation_files\")\n",
        "        annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "        self.current_video_data = self.video_utils.convert_single_video_to_images(video_path, annotation_path, self.dim)\n",
        "        for i in range(frames_missing, self.batch_size):\n",
        "          X[i,] = self.current_video_data[0][self.current_video_frame]\n",
        "          Y[i,] = self.current_video_data[1][self.current_video_frame]\n",
        "          self.current_video_frame += 1\n",
        "        \n",
        "    # We do have enough frames\n",
        "    else:\n",
        "      #####print(\"DEBUG - We have frames - taking from current only\")\n",
        "      for i in range(self.batch_size):\n",
        "        X[i,] = self.current_video_data[0][self.current_video_frame]\n",
        "        Y[i,] = self.current_video_data[1][self.current_video_frame]\n",
        "        self.current_video_frame += 1\n",
        "      \n",
        "      # If we have an exact match of video frames and have more videos\n",
        "      #####print(f\"DEBUG {self.current_video_frame} == {total_number_of_current_images} and {self.current_movie_index} <= {(len(self.movie_path_list) - 1)}\")\n",
        "      if(self.current_video_frame == total_number_of_current_images and self.current_movie_index <= (len(self.movie_path_list) - 1)):\n",
        "        #####print(f\"DEBUG - having exact match\")\n",
        "        # Continue to next video\n",
        "        self.current_movie_index += 1\n",
        "        self.current_video_frame = 0\n",
        "\n",
        "        video_path = self.movie_path_list[self.current_movie_index]\n",
        "        annotation_path = video_path.replace(\"Videos\", \"Annotation_files\")\n",
        "        annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "        self.current_video_data = self.video_utils.convert_single_video_to_images(video_path, annotation_path, self.dim)\n",
        "    \n",
        "    return X, keras.utils.np_utils.to_categorical(Y, num_classes=self.n_classes)\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of batches per epoch'\n",
        "    ## Total number of frames in all videos / batchsize\n",
        "    return int(np.floor(self.total_movie_frames / self.batch_size))\n",
        "\n",
        "\n",
        "  def __getitem__(self):\n",
        "    'Generate one batch of data'\n",
        "    \n",
        "    if(self.current_batch > self.__len__()-1):\n",
        "      self.on_epoch_end()\n",
        "\n",
        "    # Generate data\n",
        "    X, y = self.__data_generation()\n",
        "    \n",
        "    return X, y\n"
      ],
      "execution_count": 491,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5By3K9GZA9zF"
      },
      "source": [
        "### Testing data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRllNJscA_wr",
        "outputId": "d8690241-d0e7-4ad3-f203-45aa75d4980d"
      },
      "source": [
        "#utils_video\n",
        "video_utils = VideoUtils()\n",
        "\n",
        "# Datasets\n",
        "partition = video_utils.create_partition_list(p)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traning 0:152\n",
            "validation 153:190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUWc-e6pR0y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a29f52f8-8fb2-475a-b4c7-a02c8250d9f3"
      },
      "source": [
        "# Parameters\n",
        "params = {'batch_size': 32,\n",
        "          'dim': (180,180),\n",
        "          'n_channels': 3,\n",
        "          'n_classes': 2, \n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "short = partition['Training'][0][0:3]\n",
        "training_generator = FallingDataGenerator(short, partition['Training'][1], **params)\n",
        "validation_generator = FallingDataGenerator(partition['Validation'][0], partition['Validation'][1], **params)"
      ],
      "execution_count": 492,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG - on epoch end\n",
            "DEBUG - on epoch end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQwHMF9Whdko",
        "outputId": "17d137a0-7ef6-4c74-9cd0-b1acb92d1e5c"
      },
      "source": [
        "x,y = training_generator.__getitem__()\n",
        "print(f\"current movie file index : {training_generator.current_movie_index}\")\n",
        "print(f\"current video frame in movie: {training_generator.current_video_frame}\")\n",
        "print(f\"batch : {len(x)}\")"
      ],
      "execution_count": 521,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1858\n",
            "DEBUG - total_number_of_current_images : 467\n",
            "DEBUG - We have frames - taking from current only\n",
            "DEBUG 434 == 467 and 1 <= 2\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "current movie file index : 1\n",
            "current video frame in movie: 434\n",
            "batch : 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJIkNyDa9QFw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}