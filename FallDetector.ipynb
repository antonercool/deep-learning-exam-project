{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FallDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JI1ExEjqkAJp"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonercool/deep-learning-exam-project/blob/master/FallDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuijswKcNnPY"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MutW9icTjgAZ"
      },
      "source": [
        "# Deep Learning course project - Fall Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI1ExEjqkAJp"
      },
      "source": [
        "###Group members\n",
        "\n",
        "\n",
        "*   Anton Sihm, studentId.: 201504954\n",
        "*   Peter Marcus Hoveling, studentId.: 201508876\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK6SxF2bk1mh"
      },
      "source": [
        "\n",
        "###Purpose\n",
        "\n",
        "This project aims to detect human actions from live video feed, in order to achieve safety, such as\n",
        "alarms when elderly people fall in their apartments.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVy_WAXzlHvr"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5u6jEplkP8u"
      },
      "source": [
        "### Data sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mxgGvdFkd02"
      },
      "source": [
        "To evaluate the fall detection, dataset from realistic video surveillance settings will be used. Framerate\n",
        "and resolution may vary as finding the same formatted video data is challenging\n",
        "\n",
        "\n",
        "*   mViA - Fall detection dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbMKOj_WtAAT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfgWtudgrvZH"
      },
      "source": [
        "## Task 0: Mounting Google Drive data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNqOf05KhNV-",
        "outputId": "9d656999-bb3d-4f57-8c8c-d92c5713e786"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZUArads4-R"
      },
      "source": [
        "Creating a variable to the specific files mouted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88QdPuwfsoZF"
      },
      "source": [
        "from pathlib import Path\n",
        "root = '/content/gdrive/My Drive/' # Don't change this\n",
        "data_dirname = 'Data_complete' # Change as you like\n",
        "p = Path(root + data_dirname)\n",
        "p.mkdir(exist_ok=True) # should \"/content/gdrive/My Drive/Data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t--_-SBqtFTZ"
      },
      "source": [
        "Validating the data directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiApPSd8tDwI",
        "outputId": "f1cdc677-53a3-49c3-e993-9d7fca11cff9"
      },
      "source": [
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/Data_complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-MykY4QsBe0"
      },
      "source": [
        "## Task 1: Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-s2kqYj19bi"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0n6NzYQm2EwD"
      },
      "source": [
        "# Importing all necessary libraries\n",
        "import cv2\n",
        "import os\n",
        "import numpy \n",
        "import random\n",
        "\n",
        "class VideoUtils:\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def convert_single_video_to_images(self, video_path, annotation_path, dim):\n",
        "    # Read the video from specified path\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    meta_data = open(annotation_path)\n",
        "\n",
        "    start_fall_frame = meta_data.readline() \n",
        "    end_fall_frame = meta_data.readline() \n",
        "\n",
        "    not_falling = False\n",
        "    # check if has labels\n",
        "    if \",\" in start_fall_frame:\n",
        "      #print(f\"does not contain : {annotation_path}\")\n",
        "      ## skip current\n",
        "      not_falling = True\n",
        "      start_fall_frame = 0\n",
        "      end_fall_frame = 0\n",
        "    else:\n",
        "      start_fall_frame = int(start_fall_frame)\n",
        "      end_fall_frame = int(end_fall_frame)  \n",
        "\n",
        "    traning_samples = []\n",
        "    training_labels = []\n",
        "\n",
        "    currentframe = 0\n",
        "\n",
        "    while(True):\n",
        "\n",
        "      # reading from frame\n",
        "      ret,frame = video.read()\n",
        "      if ret:\n",
        "        if(frame.shape[0] != dim[0] or frame.shape[1] != dim[1]):\n",
        "          try:\n",
        "            frame = self.center_crop(frame, dim)\n",
        "          except e:\n",
        "            print(f\"failed to center from image from vid : {video_path}, error {e}\")\n",
        "\n",
        "        traning_samples.append(frame)\n",
        "\n",
        "        if not_falling == False:\n",
        "          # if falling - label 1\n",
        "          if currentframe >= start_fall_frame and currentframe <= end_fall_frame:\n",
        "            training_labels.append(1)\n",
        "          # not falling - label 0\n",
        "          else:\n",
        "            training_labels.append(0)\n",
        "        else:\n",
        "          training_labels.append(0)\n",
        "\n",
        "        # increasing counter so that it will\n",
        "        # show how many frames are created\n",
        "        currentframe += 1\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    # Release all space and windows once done\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    traning_samples = numpy.array(traning_samples)\n",
        "    training_labels =  numpy.array(training_labels)\n",
        "\n",
        "    return (traning_samples, training_labels, True)\n",
        "\n",
        "  # Function from: https://medium.com/curious-manava/center-crop-and-scaling-in-opencv-using-python-279c1bb77c74\n",
        "  def center_crop(self, img, dim):\n",
        "\t  \"\"\"Returns center cropped image\n",
        "\t  Args:\n",
        "\t  img: image to be center cropped\n",
        "\t  dim: dimensions (width, height) to be cropped\n",
        "\t  \"\"\"\n",
        "\t  width, height = img.shape[1], img.shape[0]\n",
        "\n",
        "\t  # process crop width and height for max available dimension\n",
        "\t  crop_width = dim[0] if dim[0]<img.shape[1] else img.shape[1]\n",
        "\t  crop_height = dim[1] if dim[1]<img.shape[0] else img.shape[0] \n",
        "\t  mid_x, mid_y = int(width/2), int(height/2)\n",
        "\t  cw2, ch2 = int(crop_width/2), int(crop_height/2) \n",
        "\t  crop_img = img[mid_y-ch2:mid_y+ch2, mid_x-cw2:mid_x+cw2]\n",
        "\t  return crop_img\n",
        "\n",
        "\n",
        "  def _get_movie_IDs(self, root_path):\n",
        "    list_video_paths = []\n",
        "    annotation_path = r\"/Annotation_files\"\n",
        "    video_path = r\"/Videos\"\n",
        "    \n",
        "    data_sub_dir = os.listdir(root_path)\n",
        "\n",
        "    # interate each sub dir\n",
        "    for dir in data_sub_dir:\n",
        "      if \".ipynb\" in dir:\n",
        "        continue\n",
        "      if \"model\" in data_sub_dir:\n",
        "        continue  \n",
        "      # for each file in sub dir\n",
        "      for file in os.listdir(str(root_path) + \"/\" + dir + video_path):\n",
        "        absolute_video_file_path = str(str(root_path) + \"/\" + dir + video_path + \"/\"+ file)\n",
        "        list_video_paths.append(absolute_video_file_path)\n",
        "\n",
        "    return list_video_paths\n",
        "\n",
        "\n",
        "  def normalize_image(self, image):\n",
        "    # normalizing values between 0 - 1\n",
        "    return image/255.0\n",
        "\n",
        "  # create_partition_list(p)\n",
        "  def create_partition_list(self, root_path, traning_percentage=80, validation_percentage=20):\n",
        "    partition = {}\n",
        "\n",
        "    absolute_video_path_list = video_utils._get_movie_IDs(root_path)\n",
        "    random.shuffle(absolute_video_path_list)\n",
        "\n",
        "    to_index = int((len(absolute_video_path_list)+1)*(traning_percentage/100))\n",
        "    from_index = int(len(absolute_video_path_list)*(traning_percentage/100)+1)\n",
        "\n",
        "    print(f\"traning {0}:{to_index}\")\n",
        "    print(f\"validation {from_index}:{len(absolute_video_path_list)}\")\n",
        "\n",
        "    training = absolute_video_path_list[0:to_index]\n",
        "    validation = absolute_video_path_list[from_index:]\n",
        "\n",
        "    total_frames_training = 0\n",
        "    total_frames_validation = 0\n",
        "    \n",
        "    for image_path in training:\n",
        "      cap= cv2.VideoCapture(image_path)\n",
        "      total_frames_training += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    for image_path in validation:\n",
        "      cap= cv2.VideoCapture(image_path)\n",
        "      total_frames_validation += int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    \n",
        "    partition['Training'] = (training, total_frames_training)\n",
        "    partition['Validation'] = (validation, total_frames_validation)\n",
        "\n",
        "    return partition"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPyh4Iw-1Ued"
      },
      "source": [
        "### Testing Video ultils\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxj08t9B1a3P",
        "outputId": "c275b771-c313-4dc1-f339-391dee5d3021"
      },
      "source": [
        "video_utils = VideoUtils()\n",
        "traning_samples, training_labels, is_ok = video_utils.convert_single_video_to_images(str(p) + \"/Office/Videos/video (6).avi\", str(p) + \"/Office/Annotation_files/video (6).txt\", (180,180))\n",
        "print(traning_samples.shape)\n",
        "\n",
        "absolute_video_path_list = video_utils._get_movie_IDs(p)\n",
        "\n",
        "partition = video_utils.create_partition_list(p)\n",
        "\n",
        "print(partition['Training'][1])\n",
        "print(partition['Validation'][1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(415, 180, 180, 3)\n",
            "traning 0:0\n",
            "validation 1:0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb8tV0PSkg1x"
      },
      "source": [
        "### Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLVPgdLgkodl"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from keras import utils as np_utils\n",
        "import random\n",
        "\n",
        "class FallingDataGenerator(Sequence):\n",
        "\n",
        "  def __init__(self, movie_path_list, total_movie_frames, batch_size=32, dim=(240, 320), n_channels=3, n_classes=2, shuffle=True): \n",
        "      'Initialization'\n",
        "      self.dim = dim\n",
        "      self.batch_size = batch_size\n",
        "      self.n_channels = n_channels\n",
        "      self.n_classes = n_classes\n",
        "      self.shuffle = shuffle\n",
        "      self.movie_path_list = movie_path_list\n",
        "      self.total_movie_frames = total_movie_frames\n",
        "      self.current_video_data = 0# ((415, 240, 320, 3)(415)) # 0-32, 32-64 ...... 400-415    \n",
        "      self.current_video_frame = 0\n",
        "      self.current_movie_index = 0\n",
        "      self.video_utils = VideoUtils()\n",
        "      self.current_batch = 0\n",
        "\n",
        "      # init by shuffle\n",
        "      self.on_epoch_end()\n",
        "\n",
        "  # Shuffling the order in which examples are fed to the classifier is helpful so that batches between epochs do not look alike\n",
        "  ## TODO shuffle path list\n",
        "  def on_epoch_end(self):\n",
        "    'Updates indexes after each epoch'\n",
        "    print(\"DEBUG - on epoch end\")\n",
        "    random.shuffle(self.movie_path_list)\n",
        "    self.current_video_data = 0\n",
        "    self.current_video_frame = 0\n",
        "    self.current_movie_index = 0\n",
        "\n",
        "\n",
        "  def __data_generation(self):\n",
        "    'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "    # Initialization\n",
        "    X = np.zeros((self.batch_size, *self.dim, self.n_channels)) # (32, H, W, 3)\n",
        "    Y = np.zeros((self.batch_size), dtype=int)\n",
        "\n",
        "    ## Init - first time load\n",
        "    if(self.current_video_data == 0 and self.current_video_frame == 0 and self.current_movie_index == 0):\n",
        "      #####print(\"DEBUG - First time load\")\n",
        "      video_path = self.movie_path_list[self.current_movie_index]\n",
        "      annotation_path = video_path.replace(\"Videos\", \"Annotation_files\")\n",
        "      annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "      self.current_video_data = self.video_utils.convert_single_video_to_images(video_path, annotation_path, self.dim)\n",
        "      self.current_video_data = (self.video_utils.normalize_image(self.current_video_data[0]), self.current_video_data[1])\n",
        "    \n",
        "    total_number_of_current_images = len(self.current_video_data[0])\n",
        "    #####print(f\"DEBUG - total_number_of_current_images : {total_number_of_current_images}\")\n",
        "\n",
        "    # Check if dont have enough frames\n",
        "    if(self.current_video_frame + self.batch_size + 1 > total_number_of_current_images):\n",
        "      #####print(f\"DEBUG - not have enough frames{self.current_video_frame + self.batch_size -1}>{total_number_of_current_images} - taking from current and current + 1\")\n",
        "      # How many frames did we overextend with\n",
        "      residual_value_from_current_video = (self.current_video_frame + self.batch_size)%total_number_of_current_images\n",
        "      # How many frames are we missing to complete current video\n",
        "      frames_missing = self.batch_size - residual_value_from_current_video\n",
        "\n",
        "      # loop for the remaining needed frames\n",
        "      for i in range(frames_missing):\n",
        "        #####print(f\"DEBUG - taking frame_index: {self.current_video_frame}\")\n",
        "        X[i,] = self.current_video_data[0][self.current_video_frame]\n",
        "        Y[i,] = self.current_video_data[1][self.current_video_frame]\n",
        "        self.current_video_frame += 1\n",
        "    \n",
        "      # Continue to next video\n",
        "      self.current_movie_index += 1\n",
        "      self.current_video_frame = 0\n",
        "      self.current_video_data = 0\n",
        "\n",
        "      # If we have more movies\n",
        "      # 0, 1, 2     -- 2<=2\n",
        "      if(self.current_movie_index <= (len(self.movie_path_list) - 1)):\n",
        "        #####print(\"DEBUG - taking new vid\")\n",
        "        video_path = self.movie_path_list[self.current_movie_index]\n",
        "        annotation_path = video_path.replace(\"Videos\", \"Annotation_files\")\n",
        "        annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "        self.current_video_data = self.video_utils.convert_single_video_to_images(video_path, annotation_path, self.dim)\n",
        "        self.current_video_data = (self.video_utils.normalize_image(self.current_video_data[0]), self.current_video_data[1])\n",
        "        for i in range(frames_missing, self.batch_size):\n",
        "          X[i,] = self.current_video_data[0][self.current_video_frame]\n",
        "          Y[i,] = self.current_video_data[1][self.current_video_frame]\n",
        "          self.current_video_frame += 1\n",
        "        \n",
        "    # We do have enough frames\n",
        "    else:\n",
        "      #####print(\"DEBUG - We have frames - taking from current only\")\n",
        "      for i in range(self.batch_size):\n",
        "        X[i,] = self.current_video_data[0][self.current_video_frame]\n",
        "        Y[i,] = self.current_video_data[1][self.current_video_frame]\n",
        "        self.current_video_frame += 1\n",
        "      \n",
        "      # If we have an exact match of video frames and have more videos\n",
        "      #####print(f\"DEBUG {self.current_video_frame} == {total_number_of_current_images} and {self.current_movie_index} <= {(len(self.movie_path_list) - 1)}\")\n",
        "      if(self.current_video_frame == total_number_of_current_images and self.current_movie_index <= (len(self.movie_path_list) - 1)):\n",
        "        #####print(f\"DEBUG - having exact match\")\n",
        "        # Continue to next video\n",
        "        self.current_movie_index += 1\n",
        "        self.current_video_frame = 0\n",
        "\n",
        "        video_path = self.movie_path_list[self.current_movie_index]\n",
        "        annotation_path = video_path.replace(\"Videos\", \"Annotation_files\")\n",
        "        annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "        self.current_video_data = self.video_utils.convert_single_video_to_images(video_path, annotation_path, self.dim)\n",
        "        self.current_video_data = (self.video_utils.normalize_image(self.current_video_data[0]), self.current_video_data[1])\n",
        "    \n",
        "    return X, keras.utils.np_utils.to_categorical(Y, num_classes=self.n_classes)\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the number of batches per epoch'\n",
        "    ## Total number of frames in all videos / batchsize\n",
        "    return int(np.floor(self.total_movie_frames / self.batch_size) - 10)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    'Generate one batch of data'\n",
        "    \n",
        "    if(self.current_batch > self.__len__()-1):\n",
        "      self.on_epoch_end()\n",
        "\n",
        "    # Generate data\n",
        "    X, y = self.__data_generation()\n",
        "    \n",
        "    return X, y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5By3K9GZA9zF"
      },
      "source": [
        "### Testing data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRllNJscA_wr",
        "outputId": "778b8c61-60f4-42c6-c21e-f4eafaf6d0a7"
      },
      "source": [
        "#utils_video\n",
        "video_utils = VideoUtils()\n",
        "\n",
        "# Datasets\n",
        "partition = video_utils.create_partition_list(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traning 0:0\n",
            "validation 1:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUWc-e6pR0y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfccbd53-0d01-4c3d-cad2-f0da5a25853e"
      },
      "source": [
        "# Parameters\n",
        "params = {'batch_size': 32,\n",
        "          'dim': (180,180),\n",
        "          'n_channels': 3,\n",
        "          'n_classes': 2, \n",
        "          'shuffle': True}\n",
        "\n",
        "# Generators\n",
        "#short = partition['Training'][0][0:3]\n",
        "training_generator = FallingDataGenerator(partition['Training'][0], partition['Training'][1], **params)\n",
        "validation_generator = FallingDataGenerator(partition['Validation'][0], partition['Validation'][1], **params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG - on epoch end\n",
            "DEBUG - on epoch end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQwHMF9Whdko"
      },
      "source": [
        "x,y = training_generator.__getitem__()\n",
        "print(f\"current movie file index : {training_generator.current_movie_index}\")\n",
        "print(f\"current video frame in movie: {training_generator.current_video_frame}\")\n",
        "print(f\"batch : {len(x)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZtxzB6fDiZm"
      },
      "source": [
        "### Model Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqz5bJBfDmJC",
        "outputId": "50d12013-4c4f-4ccc-a244-d56df8fc428a"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "\n",
        "# Model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(8, (3, 3), padding='same', input_shape=(180,180,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(16, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256)) # W1*x + b\n",
        "model.add(Activation('relu')) # ReLU(W1*x + b)\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2)) # number of classes\n",
        "model.add(Activation('softmax'))  # softmax(W2*x + b)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 180, 180, 8)       224       \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 180, 180, 8)       0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 90, 90, 8)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 88, 88, 16)        1168      \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 88, 88, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 44, 44, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 44, 44, 32)        4640      \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 44, 44, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 22, 22, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 22, 22, 64)        18496     \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 22, 22, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 11, 11, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 7744)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1982720   \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 256)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 514       \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,007,762\n",
            "Trainable params: 2,007,762\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMjqnrO1LTfP"
      },
      "source": [
        "### Model fit - takes around 20 mins\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC0fSExhHnXn",
        "outputId": "d1001a73-4535-4de3-fe50-d1edc7a68aa6"
      },
      "source": [
        "# Compile the model before training\n",
        "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "number_of_epochs = 2\n",
        "\n",
        "history = model.fit_generator(generator=training_generator, validation_data=validation_generator, epochs=number_of_epochs, use_multiprocessing=False)\n",
        "model.save(str(p)+ \"/model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1813/1813 [==============================] - ETA: 0s - loss: 0.2224 - accuracy: 0.9462DEBUG - on epoch end\n",
            "1813/1813 [==============================] - 599s 313ms/step - loss: 0.2224 - accuracy: 0.9462 - val_loss: 0.2027 - val_accuracy: 0.9491\n",
            "DEBUG - on epoch end\n",
            "Epoch 2/2\n",
            "1813/1813 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9462DEBUG - on epoch end\n",
            "1813/1813 [==============================] - 537s 295ms/step - loss: 0.2099 - accuracy: 0.9462 - val_loss: 0.1954 - val_accuracy: 0.9520\n",
            "DEBUG - on epoch end\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/Data_complete/model/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PskTBHYXzCS"
      },
      "source": [
        "### Evalutate - 1 vid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m-J1ILjXN-G"
      },
      "source": [
        "video_path = str(p) + \"/Coffee_room_01/Videos/video (5).avi\"\n",
        "annotation_path = str(p) + \"/Coffee_room_01/Annotation_files/video (5).txt\"\n",
        "dim = (180, 180)\n",
        "video_utils = VideoUtils()\n",
        "\n",
        "current_video_data = video_utils.convert_single_video_to_images(video_path, annotation_path, dim)\n",
        "current_video_data = (video_utils.normalize_image(current_video_data[0]), current_video_data[1])\n",
        "\n",
        "test_predictions = model.predict(current_video_data[0])\n",
        "#print(test_predictions[0])\n",
        "\n",
        "#for i, prediction in enumerate(test_predictions):\n",
        "#  print(prediction)\n",
        "#  print(f\"Current video data: {current_video_data[1][i]}\")\n",
        "\n",
        "\n",
        "#test_acc, test_loss = model.evaluate(current_video_data[0], current_video_data[1], verbose=2)\n",
        "#print(test_acc)\n",
        "#print(test_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BucITFod3Wa"
      },
      "source": [
        "### Dataset review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPsWJuaSqoA9"
      },
      "source": [
        "### Falling vs. not-falling class distribution - 9 mins to run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2etTd12eCUl",
        "outputId": "349d08a5-36a4-4f2f-a82e-c7086730f272"
      },
      "source": [
        "video_utils = VideoUtils()\n",
        "\n",
        "# Falling vs. not-falling class distribution       //9 mins to run\n",
        "total_movies_to_fetch = video_utils._get_movie_IDs(p)\n",
        "\n",
        "print(f\"Total movies: {len(total_movies_to_fetch)}\")\n",
        "label_0_count = 0\n",
        "label_1_count = 0\n",
        "\n",
        "\n",
        "for movie in total_movies_to_fetch:\n",
        "  annotation_path = movie.replace(\"Videos\", \"Annotation_files\")\n",
        "  annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "\n",
        "  samples, labels, _ = video_utils.convert_single_video_to_images(movie, annotation_path, (180, 180))\n",
        "\n",
        "  label_0_count += labels.tolist().count(0)\n",
        "  label_1_count += labels.tolist().count(1)\n",
        "\n",
        "print(f\"Total number of 'Not-Falling' samples: {label_0_count}\")\n",
        "print(f\"Total number of 'Falling' samples: {label_1_count}\")\n",
        "print(f\"{(label_1_count/(label_1_count + label_0_count))*100}% is falling\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total movies: 190\n",
            "Total number of 'Not-Falling' samples: 71883\n",
            "Total number of 'Falling' samples: 4028\n",
            "Falling dataset %: 5.306213855699438%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmTSSePeraGi"
      },
      "source": [
        "### confusion matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6lhelk1vUdn"
      },
      "source": [
        "model = keras.models.load_model(str(p) + \"/model\")\n",
        "\n",
        "\n",
        "test_paths  = partition['Validation'][0]\n",
        "\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "for test_path in test_paths:\n",
        "  annotation_path = test_path.replace(\"Videos\", \"Annotation_files\")\n",
        "  annotation_path = annotation_path.replace(\".avi\", \".txt\")\n",
        "  samples, current_vid_labels, _ = video_utils.convert_single_video_to_images(test_path, annotation_path, (180, 180))\n",
        "  samples = video_utils.normalize_image(samples)\n",
        "  probabilities = model.predict(samples)\n",
        "  current_vid_predictions = np.argmax(probabilities,axis=1)\n",
        "\n",
        "  for i in range(len(current_vid_labels)):\n",
        "    true_labels.append(current_vid_labels[i])\n",
        "    predicted_labels.append(current_vid_predictions[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "g_BsVVUOrcz3",
        "outputId": "72e0f609-b497-480e-faa4-f5cb85be4d41"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "confusion_matix  = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matix,\n",
        "                              display_labels=[\"not falling\", \"falling\"])\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr7wk9xjy0Mx"
      },
      "source": [
        "The confusion matrix shows that the model predicts \"not falling 100%\" and allways confuses falling as \"not falling\"\n",
        "* Total number of 'Not-Falling' samples: 71883\n",
        "* Total number of 'Falling' samples: 4028\n",
        "*  5.306213855699438 % of the data is falling \n",
        "\n",
        "Very imbalanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmGjH62Tctmo"
      },
      "source": [
        "### TODO\n",
        "* **Search about class imbalance**\n",
        "  * Binary classifier, where we can edit wiegted loss W_1*l0 + W_2*l1 where W_2 counts for more since, we dont have that much falling data\n",
        "* **Modify own datagenereator**\n",
        "  * Oversampling/undersampling\n",
        "  * Oversampling: **bold text** Instead of taking next 32 frames, we should take 16,16 of each class(falling/not falling) and oversampling from the falling distribution, since we dont have that many\n",
        "  * **Data augmentation**:\n",
        "  * Augment the falling distribution since we dont have that many samples\n",
        "* **Look into what crossvalidation is**"
      ]
    }
  ]
}